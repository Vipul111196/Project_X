{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716184e9a2059464",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T01:58:55.830793Z",
     "start_time": "2024-09-09T01:58:55.826549Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e6c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6270b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "env_name = os.environ.get('CONDA_DEFAULT_ENV')\n",
    "print(f\"Active Conda environment: {env_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6612e1d6f514a41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T01:38:51.353458Z",
     "start_time": "2024-09-09T01:38:50.165426Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\")\n",
    "# nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e934166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting Current Directory\n",
    "\n",
    "import os\n",
    "print(os.getcwd())  # Get current working directory\n",
    "print(os.listdir(\".\"))  # List all files in the current directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c8e3241afa188",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-09T00:48:26.928937Z",
     "start_time": "2024-09-09T00:48:26.924701Z"
    }
   },
   "outputs": [],
   "source": [
    "# Open the file in read mode\n",
    "with open(\"I:/11_DFKI_Hiwi/Work/01_Code/Graphusion/inputs/abstracts.txt\", \"r\") as file:\n",
    "    # Read all lines into a list\n",
    "    texts = file.readlines()\n",
    "\n",
    "# Remove trailing newline characters from each line\n",
    "texts = [line.strip() for line in texts]\n",
    "\n",
    "print(len(texts))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a835e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the list\n",
    "for line in texts[0:5]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2a74ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create BERTopic Extractor\n",
    "umap_model=UMAP(n_neighbors=20,n_components=50,metric=\"cosine\",min_dist=0.0,random_state=37)\n",
    "vectorizer_model=CountVectorizer(ngram_range=(2,4),stop_words=\"english\")\n",
    "ctfidf_model=ClassTfidfTransformer(reduce_frequent_words=False)\n",
    "sentence_model=SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "topic_model=BERTopic(verbose=True,\n",
    "                     umap_model=umap_model,\n",
    "                     ctfidf_model=ctfidf_model,\n",
    "                     vectorizer_model=vectorizer_model,\n",
    "                     embedding_model=sentence_model,\n",
    "                     representation_model=representation_model,\n",
    "                     nr_topics=50,\n",
    "                     low_memory=True,\n",
    "                     calculate_probabilities=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8657695",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, _ = topic_model.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a43850",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_topics = topic_model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts=[]\n",
    "\n",
    "for topic_num, keywords in all_topics.items():\n",
    "    if topic_num != -1:\n",
    "        topic_keywords = [word for word, value in keywords]\n",
    "        concepts.extend(topic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5706ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicates\n",
    "concepts = list(set(keyword.lower() for keyword in concepts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01bef23",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a98f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"extracted_concepts.tsv\", \"w\") as f:\n",
    "    for id, concept in enumerate(concepts, 1):\n",
    "        f.write(f\"{id}|{concept}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796d6b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_concepts = pd.read_csv(\"extracted_concepts.tsv\", delimiter=\"|\", header=None)\n",
    "extracted_concepts = extracted_concepts[1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f64f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f37ce49",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def singularize_concept(concept):\n",
    "    words = concept.split()\n",
    "    singular_words = [lemmatizer.lemmatize(word, wordnet.NOUN) for word in words]\n",
    "    return ' '.join(singular_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd00b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# singularize concepts\n",
    "extracted_concept = [singularize_concept(concept) for concept in extracted_concepts]\n",
    "\n",
    "# convert to lowercase\n",
    "extracted_concept = [concept.lower() for concept in extracted_concept]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc481486",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(extracted_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305746ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"extracted_concepts_upd.tsv\", \"w\") as f:\n",
    "    for id, concept in enumerate(concepts, 1):\n",
    "        f.write(f\"{id}|{concept}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55569411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataframe (column label indicated the source of the concept: 0=extracted, 1=gold)\n",
    "df_old = pd.DataFrame(extracted_concept, columns=[\"concept\"])\n",
    "df_old[\"label\"] = 0\n",
    "\n",
    "# df_new = pd.DataFrame(gold_concept, columns=[\"concept\"])\n",
    "# df_new[\"label\"] = 1\n",
    "\n",
    "# df = pd.concat([df_old, df_new])\n",
    "# df = df.sort_values(by=\"label\")\n",
    "\n",
    "df = df_old.sort_values(by=\"label\")\n",
    "df = df.drop_duplicates(subset=\"concept\", keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85095cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72622dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the text dataset to only texts containing the concepts\n",
    "\n",
    "def filter_abstracts_by_term(term, abstracts, threshold=70):\n",
    "    filtered_abstracts = []\n",
    "    for abstract in abstracts:\n",
    "        if isinstance(abstract, str):\n",
    "            if fuzz.partial_ratio(term.lower(), abstract.lower()) >= threshold:\n",
    "                filtered_abstracts.append(abstract)\n",
    "    return filtered_abstracts\n",
    "\n",
    "concept_abstracts = {}\n",
    "for index, row in tqdm(df.iterrows(), desc=\"Processing concepts\", total=df.shape[0]):\n",
    "    concept = row[\"concept\"]\n",
    "    label = row[\"label\"]\n",
    "    filtered_abstracts = filter_abstracts_by_term(concept, texts)\n",
    "    concept_abstracts[concept] = {\n",
    "        \"abstracts\": filtered_abstracts,\n",
    "        \"label\": label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653d0047",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_0_count = sum(1 for details in concept_abstracts.values() if details['label'] == 0)\n",
    "print(f\"Number of concepts added through BERTopic: {label_0_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84146580",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_abstracts_count = sum(1 for details in concept_abstracts.values() if not details['abstracts'])\n",
    "print(f\"Number of concepts with empty filtered_abstracts: {empty_abstracts_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b75b658",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = \"concept_abstracts.json\"\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(concept_abstracts, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae82e114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check if the file was saved correctly\n",
    "\n",
    "with open(output_file_path, 'r', encoding='utf-8') as f:\n",
    "    loaded_concept_abstracts = json.load(f)\n",
    "\n",
    "print(f\"Number of concepts in loaded file: {len(loaded_concept_abstracts)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "KGRag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
